\documentclass[conference]{IEEEtran}
\usepackage[english,portuguese]{babel}
\usepackage{blindtext, graphicx}
\usepackage{listings}
\lstset{
  language=C++,
  numbers=left,
  breaklines=true,
  xleftmargin=4em,
  resetmargins=true,
  basicstyle=\footnotesize,
  numberstyle=\footnotesize,
}
\usepackage{graphicx}
\usepackage[font=small]{caption}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{mathtools}
\usepackage{float}

\selectlanguage{portuguese}

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\title{Diagnóstico da doença de Alzheimer usando imagens de ressonância magnética por rede neural convolucional}

\author{
  \IEEEauthorblockN{Marlon Valmórbida Cendron}
  \IEEEauthorblockA{
    \textit{Ciência da Computação, IFC}\\
    Videira, Brasil\\
    marlonvcendron@gmail.com}
}

\begin{document}

\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}

%%% %%% Abstract

\begin{abstract}
  Em teoria da informação, a entropia mede a quantidade de informação de um determinado sistem. Na neurociência, a entropia pode ser usada para analisar a dinâmica dos neurônios. No entanto, permanece desconhecida uma maneira de como medir quantitativamente a entropia de populações neuronais. Esse artigo propõe um método para calcular tal entropia em tarefas de tomada de decisão, e para tal, são analisados dados de ativação neuronal do cérebro de ratos capturados usando Neuropixels durante uma simples tarefa de discirminação visual. Um modelo foi então desenvolvido para calcular a entropia a partir da probabilidade de frequência de ativação dos neurônios. Pôde-se então analisar a entropia e notar diferenças do resultado em diferentes áreas do cérebro, para diferentes estímulos e para quando o rato respondia corretamente ou não.
\end{abstract}
  
\global\long\def\IEEEkeywordsname{Palavras-chave}

\begin{IEEEkeywords}
 Entropia, Neurônio, Baseline.
\end{IEEEkeywords}

\selectlanguage{english}
\begin{abstract}
  In information theory, entropy measures the amount of information in a given system. In neuroscience, entropy can be used to analyze the dynamics of neurons. However, a way to quantitatively measure the entropy of neuronal populations remains unknown. This article proposes a method to calculate such entropy in decision-making tasks, and for that, we analyze neuronal activation data from the brain of rats captured using Neuropixels during a simple visual discrimination task. A model was then developed to calculate entropy from the probability of activation frequency of neurons. It was then possible to analyze entropy and note differences in the result in different areas of the brain, for different stimuli and for when the rat answered correctly or not.
\end{abstract}
  
\global\long\def\IEEEkeywordsname{Keywords}

\begin{IEEEkeywords}
  Entropy, Neuron, Baseline.
\end{IEEEkeywords}

\selectlanguage{portuguese}

%%% %%% Intro

\section{Introdução}

A entropia de Shannon mede a informação de um sistema a partir da distribuição de probabilidade p(x) de seus possíveis estados $\chi$. Uma distribuição centrada possui entropia nula, ao passo que uma distribuição randômica contém entropia máxima. Em neurociência, podemos usar a entropia para representar o fluxo de informação associado à atividade de populações de neurônios, sendo necessária a obtenção da distribuição de probabilidade do trem de disparos neuronais. Entretanto, determinar p(x) é um problema de difícil tratamento matemático, dado que as distribuições envolvem múltiplas variáveis dependentes entre si.

\section{Fundamentação teórica}

\subsection{Entropia}

Entropia é um conceito amplamente utilizado em diversas áreas, principalmente em áreas da física como a termodinâmica. É um conceito relacionado com um estado de desordem, aleatoriedade ou incerteza. Na teoria da informação, a entropia de uma variável aleatória é o nível médio de informação ou surpresa.

Por exemplo, o resultado de jogar um dado justo (em que cada lado tem a mesma probabilidade de ficar para cima) tem entropia maior do que um dado alterado que tem probabilidade de 90\% de ter o resultado 6. Isso porque existe muito menos surpresa com o resultado do dado alterado, e, portanto, menos informação: é esperado que caia o número 6.

Quantitativamente, é utilizada a equação de entropia de Shannon (\refeq{eqn:shannon}) para definir quanta entropia uma variável aleatória possui\cite{shannon}.

\begin{equation}
\label{eqn:shannon}
H(X)  = - \sum_{x \in \chi} p(x) \log p(x) \\
\end{equation}
onde:
\begin{conditions}
H (X) & entropia de Shannon em bits (se usado $\log_2$) \\
X     & variável aleatória \\   
\chi  & alfabeto da variável $X$ \\
p(x)  & probabilidade de $x$
\end{conditions}

A figura \ref{fig:entropy} exemplifica 3 casos com valores diferentes de entropia. No primeiro caso, só há um único resultado possível com 100\% de chance de ocorrer, portanto nunca há surpresa alguma em ver esse resultado, não há nenhuma surpresa e nenhuma informação nova é adquirida, por esse motivo a entropia é igual a zero. No segundo caso, todos os resultados são igualmente prováveis e a entropia é máxima, tendo o valor de $\log n$ para $n$ resultados possíveis. O último cenário exemplifica uma variável aleatória onde cada resultado tem probabilidades diferentes.

% \begin{figure}[h!]
% \centering
% \includegraphics[width=8.5cm]{entropy.png}
% \caption{Exemplo do valor da entropia para variáveis aleatórias que se comportam de maneiras diferentes}
% \label{fig:entropy}
% \end{figure}

\subsection{Neurônios}

Neurônios são células que se comunicam entre si ou entre outros tipos de células através das sinapses. Na comunicação entre neurônios do cérebro, o neurônio recebe sinais através das sinapses pelos seus dendritos, e caso haja excitação elétrica o suficiente por conta desses sinais, o neurônio dispara um potencial de ação (sinal) pelo seu axônio para os dendritos de outros neurônios. Cerca de 86 bilhões\cite{houzel} de neurônios se comunicam dessa forma entre si para processar todas as informações sensoriais, mover os membros, pensar e tudo mais que nossos cérebros são capazes. 

\subsection{Hipótese do cérebro entrópico}

A entropia do cérebro vem sendo cada vez mais analisada em estudos recentes\cite{fractal}, principalmente pelo polêmico trabalho da Hipótese do cérebro entrópico\cite{entropic}, postulado por Robin Carhart-Harris

A hipótese do cérebro entrópico é um modelo de função cerebral que postula uma relação entre a entropia no cérebro e estados psicológicos. O cérebro humano tende a operar logo abaixo de um estado de criticidade. A criticidade é um estado de comportamento complexo em um sistema entre ordem pura e estados aleatórios puros. Ou seja, nossas mentes são ordenadas apenas o suficiente para fazer as coisas que precisamos regularmente.

Pequenas flutuações na entropia cerebral são naturais, mas de acordo com o modelo de Carhart-Harris, muitos estados conscientes podem ser classificados como ``baixa entropia'' ou ``alta entropia''. Os resultados de ressonância magnética do estudo sugerem que psicodélicos aumentam as conexões gerais no córtex, desorganizando assim a atividade cerebral. Muitos estados que afetam o córtex temporal medial de maneira semelhante provavelmente atingem essas características da mesma maneira. Da mesma forma, pode-se argumentar que estados que exibem conexão cortical mais baixa estariam associados a pensamentos e comportamentos mais rígidos, algo que podemos ver em condições como depressão clínica, vício e TOC.

\section{Métodos}

Nesse trabalho, são analisados os dados de Neuropixels da ativação neuronal de ratos perfomando uma tarefa de discriminação visual de Steinmetz et. al\cite{steinmetz}. Neuropixels são sondas implantadas no cérebro capazes de gravar a ativação neuronal de centenas de neurônios ao mesmo tempo, como ilustra a Figura \ref{fig:neuropixel}.

% \begin{figure}[h!]
% \centering
% \includegraphics[width=3.5cm]{neuropixel.png}
% \caption{Neuropixel no cérebro de um rato}
% \label{fig:neuropixel}
% \end{figure}


% \begin{figure}[h!]
% \centering
% \includegraphics[width=6.5cm]{test.png}
% \caption{Imagens mostradas e possibilidades de escolha do rato}
% \label{fig:test}
% \end{figure}

Os dados coletados são referentes a 2500ms de atividade cerebral: onde os primeiros 500ms correspondem à atividade pré-estímulo visual e os 2000ms restantes, pós-estímulo. Dessa forma, na análise feita, os dados dos primeiros 50ms são usados como \textit{baseline}. Os dados são organizados em 250 janelas temporais de 10ms cada, contendo a quantidade de vezes que o neurônio disparou nesse tempo.

A entropia de cada região cortical e subcortical foi calculada a partir da distribuição de probabilidade de disparo de cada neurônio, em diferentes tentativas e independentemente do tempo. Para obter as distribuições de probabilidade, as médias de disparos dos neurônios foram inicialmente calculadas para cada janela temporal e para cada tentativa do rato.

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=15cm]{prob.png}
% \caption{Probabilidade de ativação neuronal (Y) para cada momento da tentativa (X)}
% \label{fig:prob}
% \end{figure*}

Com essas médias, diferentes frequências foram extraídas através de um histograma, que resulta em uma distribuição de probabilidade ao ser dividido pelo total de médias por tentativa. A distribuição de probabilidade para a tentativa de número um está ilustrada no gráfico da Figura \ref{fig:prob}. Note que, no exemplo da figura, a maior probabilidade de ativação está nos 500ms (0.05 no gráfico). A partir de uma distribuição de probabilidade, é então possível calcular a entropia da distribuiçaõ e compará-la com a de tentativas cujas respostas do animal foram corretas ou incorretas, assim como para diferentes direções do estímulo.


\section{Resultados}

Para acertos, estímulos do lado contralateral ao córtex visual apresentaram variação de entropia positiva quando comparados pré- e pós-estímulo (Mann-Whitney, p=0,05), ao passo que sub-regiões no mesencéfalo, ligadas ao controle motor, manifestaram aumento de entropia independentemente da localização do estímulo correto (esquerda p=0,015; centro p=0,04; direita p=0,01). A entropia de áreas como um todo não variou (p>0,05), tanto para respostas corretas quanto incorretas.

O gráfico da Figura \ref{fig:correct} ilustra a entropia para cada área e para diferentes estímulos, onde Middle seria o caso NoGo, e baseline é a entropia dos 500ms pré-estímulo. A entropia é maior para todos os casos e é possível ver alguma diferença dependendo da direção do estímulo. O gráfico da Figura \ref{fig:wrong} ilustra a mesma coisa, mas para os casos em que o rato errou a resposta.

% \begin{figure}[H]
% \centering
% \includegraphics[width=8.5cm]{correct.png}
% \caption{Entropia por região e estímulo para respostas corretas}
% \label{fig:correct}
% \end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=8.5cm]{wrong.png}
% \caption{Entropia por região e estímulo para respostas erradas}
% \label{fig:wrong}
% \end{figure}

\section{Conclusão}

Devido a não haver nenhum método bem estabelecido de como calcular a entropia do cérebro, é importante explorar novas maneiras de fazer isso e para diferentes tipos de dados.
O método descrito estabelece um modelo de obtenção e testagem da entropia de populações neuronais durante tomada de decisão. Assim, correlatos comportamentais a partir da análise de entropia puderam ser analisados. Pequenas diferenças de entropia foram comprovadas utilizando esse método. Para o futuro, pode ser analisada a possibilidade de, a partir da entropia calculada, prever o comportamento do rato ou, seguindo a ideia inicial da hipótese do cérebro entrópico, analisar os diferentes estados de consciência e atenção do rato e como isso afeta nos resultados. 

\section*{Agradecimentos}

Agradeço ao Claudionei Lovato pela ideia do trabalho.

\bibliographystyle{plain}  
\bibliography{refs}

\end{document}
